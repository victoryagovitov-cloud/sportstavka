"""
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä MarathonBet —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—ã–ø–∞–¥–∞—é—â–∏—Ö —Å–ø–∏—Å–∫–æ–≤ (ecids)
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
"""

import logging
import re
import time
from typing import List, Dict, Any, Optional
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class MarathonBetExpandedScraper:
    """
    –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö MarathonBet
    
    –û–°–û–ë–ï–ù–ù–û–°–¢–ò:
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—ã–ø–∞–¥–∞—é—â–∏—Ö —Å–ø–∏—Å–∫–æ–≤ (ecids –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: –∫–æ–º–∞–Ω–¥—ã + —Å—á–µ—Ç–∞ + –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã + –ª–∏–≥–∞
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –º–∞—Ç—á–µ–π
    - –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥ (–æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫)
    """
    
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.driver = None
        
        # –ë–∞–∑–æ–≤—ã–µ URL –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤ —Å–ø–æ—Ä—Ç–∞
        self.base_urls = {
            'football': 'https://www.marathonbet.ru/su/live/26418',
            'tennis': 'https://www.marathonbet.ru/su/live/22723',
            'table_tennis': 'https://www.marathonbet.ru/su/live/414329',
            'handball': 'https://www.marathonbet.ru/su/live/26422'
        }
        
    def _setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Selenium –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        if self.driver:
            return
            
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.logger.info("‚úÖ Selenium –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è expanded –≤–µ—Ä—Å–∏–∏ –∑–∞–ø—É—â–µ–Ω")
    
    def get_all_expanded_matches(self, sport: str = 'football') -> List[Dict[str, Any]]:
        """
        –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –º–∞—Ç—á–∏ –≤–∫–ª—é—á–∞—è —Å–∫—Ä—ã—Ç—ã–µ –≤ –≤—ã–ø–∞–¥–∞—é—â–∏—Ö —Å–ø–∏—Å–∫–∞—Ö
        
        Args:
            sport: –í–∏–¥ —Å–ø–æ—Ä—Ç–∞
            
        Returns:
            List[Dict]: –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –º–∞—Ç—á–µ–π
        """
        
        if sport not in self.base_urls:
            self.logger.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Å–ø–æ—Ä—Ç: {sport}")
            return []
        
        base_url = self.base_urls[sport]
        self.logger.info(f"üîç –ü–æ–ª—É—á–∞–µ–º expanded –¥–∞–Ω–Ω—ã–µ {sport}: {base_url}")
        
        try:
            self._setup_driver()
            
            # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ ecids
            ecids = self._discover_all_ecids(base_url)
            
            # –®–∞–≥ 2: –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞—Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏
            all_matches = self._get_matches_with_expanded_lists(base_url, ecids)
            
            self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(all_matches)} expanded –º–∞—Ç—á–µ–π")
            return all_matches
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è expanded –º–∞—Ç—á–µ–π: {e}")
            return []
    
    def _discover_all_ecids(self, base_url: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ ecids –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        
        ecids = []
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            self.driver.get(base_url)
            time.sleep(3)
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å ecids
            # –û–±—ã—á–Ω–æ —ç—Ç–æ –∫–Ω–æ–ø–∫–∏ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è —Å–ø–∏—Å–∫–æ–≤ –∏–ª–∏ data-–∞—Ç—Ä–∏–±—É—Ç—ã
            
            # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –≤ HTML –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            elements_with_data = self.driver.find_elements(By.XPATH, '//*[@data-event-id or @data-ecid or @data-id]')
            
            for element in elements_with_data:
                try:
                    for attr in ['data-event-id', 'data-ecid', 'data-id']:
                        value = element.get_attribute(attr)
                        if value and value.isdigit():
                            ecids.append(value)
                except:
                    continue
            
            # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –≤ JavaScript –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            page_source = self.driver.page_source
            js_ecids = re.findall(r'ecid[s]?["\']?\\s*[:=]\\s*["\']?(\\d+)', page_source)
            ecids.extend(js_ecids)
            
            # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤ URL —Å—Å—ã–ª–æ–∫
            links = self.driver.find_elements(By.TAG_NAME, 'a')
            for link in links:
                try:
                    href = link.get_attribute('href')
                    if href and 'ecids=' in href:
                        ecids_in_url = re.findall(r'ecids=([\\d,]+)', href)
                        for ecids_string in ecids_in_url:
                            ecids.extend(ecids_string.split(','))
                except:
                    continue
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_ecids = list(set(ecids))
            
            self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(unique_ecids)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ecids")
            return unique_ecids[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ ecids: {e}")
            return []
    
    def _get_matches_with_expanded_lists(self, base_url: str, ecids: List[str]) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–∞—Ç—á–∏ —Å —Ä–∞—Å–∫—Ä—ã—Ç—ã–º–∏ –≤—ã–ø–∞–¥–∞—é—â–∏–º–∏ —Å–ø–∏—Å–∫–∞–º–∏"""
        
        all_matches = []
        
        if not ecids:
            self.logger.warning("‚ùå –ù–µ—Ç ecids –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è —Å–ø–∏—Å–∫–æ–≤")
            return []
        
        # –°–æ–∑–¥–∞–µ–º URL —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏–º–∏—Å—è —Å–ø–∏—Å–∫–∞–º–∏
        expanded_urls = []
        
        # –û–¥–∏–Ω–æ—á–Ω—ã–µ ecids
        for ecid in ecids[:5]:  # –ü–µ—Ä–≤—ã–µ 5
            expanded_urls.append(f"{base_url}?ecids={ecid}")
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ ecids
        if len(ecids) >= 2:
            expanded_urls.append(f"{base_url}?ecids={','.join(ecids[:2])}")
        if len(ecids) >= 3:
            expanded_urls.append(f"{base_url}?ecids={','.join(ecids[:3])}")
        if len(ecids) >= 5:
            expanded_urls.append(f"{base_url}?ecids={','.join(ecids[:5])}")
        
        self.logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º {len(expanded_urls)} expanded URL")
        
        for i, url in enumerate(expanded_urls):
            try:
                self.logger.info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º expanded URL {i+1}: {url}")
                
                self.driver.get(url)
                time.sleep(4)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Ç—á–∏ –∏–∑ expanded —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                matches = self._extract_matches_from_expanded_page(url)
                
                if matches:
                    self.logger.info(f"‚úÖ –ò–∑ URL {i+1} –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(matches)} –º–∞—Ç—á–µ–π")
                    all_matches.extend(matches)
                else:
                    self.logger.debug(f"‚ùå –ò–∑ URL {i+1} –º–∞—Ç—á–∏ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
                    
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ expanded URL {i+1}: {e}")
                continue
        
        # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
        unique_matches = self._deduplicate_expanded_matches(all_matches)
        
        return unique_matches
    
    def _extract_matches_from_expanded_page(self, url: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞—Ç—á–∏ –∏–∑ expanded —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        matches = []
        
        try:
            # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã (–æ—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö)
            tables = self.driver.find_elements(By.TAG_NAME, 'table')
            
            for table_index, table in enumerate(tables):
                try:
                    rows = table.find_elements(By.TAG_NAME, 'tr')
                    
                    for row_index, row in enumerate(rows):
                        try:
                            cells = row.find_elements(By.TAG_NAME, 'td')
                            
                            if len(cells) >= 5:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —è—á–µ–µ–∫ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –º–∞—Ç—á–∞
                                match_data = self._parse_table_row(cells, url, table_index, row_index)
                                if match_data:
                                    matches.append(match_data)
                        except:
                            continue
                except:
                    continue
            
            return matches
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ expanded —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return []
    
    def _parse_table_row(self, cells, url: str, table_index: int, row_index: int) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–∞—Ç—á–∞"""
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —è—á–µ–µ–∫
            cell_texts = []
            for cell in cells:
                cell_text = cell.text.strip()
                if cell_text:
                    cell_texts.append(cell_text)
            
            if len(cell_texts) < 3:
                return None
            
            # –ü–µ—Ä–≤–∞—è —è—á–µ–π–∫–∞ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç: —Å—á–µ—Ç, –≤—Ä–µ–º—è, –∫–æ–º–∞–Ω–¥—ã
            main_cell = cell_texts[0]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            score_match = re.search(r'(\\d+):(\\d+)', main_cell)
            time_match = re.search(r'(\\d{1,2}:\\d{2})', main_cell)
            
            if not score_match:
                return None
            
            score = score_match.group(0)
            match_time = time_match.group(0) if time_match else 'LIVE'
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–∞–Ω–¥—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π —è—á–µ–π–∫–∏
            teams = self._extract_teams_from_main_cell(main_cell)
            
            if not teams or len(teams) < 2:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∏–∑ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —è—á–µ–µ–∫
            coefficients = []
            for cell_text in cell_texts[1:]:
                # –ò—â–µ–º —á–∏—Å–ª–∞ –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
                coeff_matches = re.findall(r'\\b(\\d+\\.\\d+)\\b', cell_text)
                coefficients.extend(coeff_matches)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            match_data = {
                'team1': teams[0],
                'team2': teams[1],
                'score': score,
                'time': match_time,
                'source': 'marathonbet_expanded',
                'source_url': url,
                'table_position': f'table_{table_index}_row_{row_index}',
                'raw_main_cell': main_cell
            }
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            if len(coefficients) >= 6:
                match_data['odds'] = {
                    '1': coefficients[0] if len(coefficients) > 0 else None,
                    'X': coefficients[1] if len(coefficients) > 1 else None, 
                    '2': coefficients[2] if len(coefficients) > 2 else None,
                    '1X': coefficients[3] if len(coefficients) > 3 else None,
                    '12': coefficients[4] if len(coefficients) > 4 else None,
                    'X2': coefficients[5] if len(coefficients) > 5 else None
                }
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞–≤–∫–∏
                if len(coefficients) > 6:
                    match_data['handicap'] = coefficients[6:8]
                    match_data['total'] = coefficients[8:10] if len(coefficients) > 8 else []
            
            return match_data
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
            return None
    
    def _extract_teams_from_main_cell(self, main_cell: str) -> Optional[List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π —è—á–µ–π–∫–∏"""
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        lines = main_cell.split('\\n')
        
        team_candidates = []
        for line in lines:
            line = line.strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–≤–∏–¥–Ω–æ –Ω–µ –∫–æ–º–∞–Ω–¥—ã
            if (line and 
                not re.match(r'^\\d+$', line) and  # –ù–µ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
                not re.match(r'^\\d+\\.\\d+$', line) and  # –ù–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
                not re.match(r'^\\d+:\\d+$', line) and  # –ù–µ –≤—Ä–µ–º—è/—Å—á–µ—Ç
                not line in ['1.', '2.', '+', '-', 'X'] and  # –ù–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                len(line) >= 3):
                
                team_candidates.append(line)
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ –∫–æ–º–∞–Ω–¥—ã
        if len(team_candidates) >= 2:
            team1 = self._clean_team_name(team_candidates[0])
            team2 = self._clean_team_name(team_candidates[1])
            
            if team1 and team2 and team1.lower() != team2.lower():
                return [team1, team2]
        
        return None
    
    def _clean_team_name(self, name: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        if not name:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –≤ –Ω–∞—á–∞–ª–µ (1., 2.)
        name = re.sub(r'^\\d+\\.\\s*', '', name)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        name = re.sub(r'[^\\w\\s,.-]', '', name).strip()
        name = re.sub(r'\\s+', ' ', name)
        
        return name
    
    def _deduplicate_expanded_matches(self, matches: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã expanded –º–∞—Ç—á–µ–π"""
        
        seen = set()
        unique = []
        
        for match in matches:
            key = f"{match.get('team1', '').lower()}_{match.get('team2', '').lower()}_{match.get('score', '')}"
            if key not in seen and len(key) > 10:
                seen.add(key)
                unique.append(match)
        
        return unique
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –±—Ä–∞—É–∑–µ—Ä"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("üîß Expanded –±—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
            except:
                pass
            finally:
                self.driver = None


# –§—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_expanded_scraper():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ expanded –ø–∞—Ä—Å–µ—Ä–∞"""
    
    import logging
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    scraper = MarathonBetExpandedScraper(logger)
    
    try:
        print("\\nüöÄ –¢–ï–°–¢–ò–†–£–ï–ú EXPANDED –ü–ê–†–°–ï–†:")
        print("="*40)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ—É—Ç–±–æ–ª
        football_matches = scraper.get_all_expanded_matches('football')
        
        if football_matches:
            print(f"\\n‚úÖ –ù–ê–ô–î–ï–ù–û {len(football_matches)} EXPANDED –ú–ê–¢–ß–ï–ô!")
            
            for i, match in enumerate(football_matches[:3], 1):
                print(f"\\nüìä –ú–ê–¢–ß {i} (–ø–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ):")
                print("="*60)
                
                print(f"üèüÔ∏è –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
                print(f"   –ö–æ–º–∞–Ω–¥–∞ 1: {match.get('team1')}")
                print(f"   –ö–æ–º–∞–Ω–¥–∞ 2: {match.get('team2')}")
                print(f"   –°—á–µ—Ç: {match.get('score')}")
                print(f"   –í—Ä–µ–º—è: {match.get('time')}")
                
                odds = match.get('odds', {})
                if odds:
                    print(f"\\nüí∞ –ö–û–≠–§–§–ò–¶–ò–ï–ù–¢–´:")
                    print(f"   1: {odds.get('1')} | X: {odds.get('X')} | 2: {odds.get('2')}")
                    print(f"   1X: {odds.get('1X')} | 12: {odds.get('12')} | X2: {odds.get('X2')}")
                
                handicap = match.get('handicap', [])
                total = match.get('total', [])
                
                if handicap:
                    print(f"\\nüìà –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –°–¢–ê–í–ö–ò:")
                    print(f"   –§–æ—Ä–∞: {handicap}")
                if total:
                    print(f"   –¢–æ—Ç–∞–ª: {total}")
                
                print(f"\\nüîß –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
                print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {match.get('source')}")
                print(f"   URL: {match.get('source_url')}")
                print(f"   –ü–æ–∑–∏—Ü–∏—è: {match.get('table_position')}")
        
        else:
            print("\\n‚ùå Expanded –º–∞—Ç—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
    finally:
        scraper.close()


if __name__ == "__main__":
    test_expanded_scraper()